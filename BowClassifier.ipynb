{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec19261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783c6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f139e78",
   "metadata": {},
   "source": [
    "# Exploritory Data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfb7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_json(path_or_buf=\"./data/domain1_train.json/domain1_train.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "414a94f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     [70, 746, 825, 109, 2083, 0, 2, 0, 0, 0, 9, 0,...\n",
       "label                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f9668771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 746, 825, 109, 2083, 0, 2, 0, 0, 0, 9, 0, 1004, 19, 0, 0, 7, 913]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cfba73ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392de681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[70, 746, 825, 109, 2083, 0, 2, 0, 0, 0, 9, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1209, 179, 1952, 4, 4959, 7, 0, 2, 978, 1522,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[287, 3, 3330, 0, 23, 12, 13, 465, 74, 8, 0, 8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 3, 592, 19, 2, 706, 1439, 2575, 7, 2, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9, 2, 110, 12, 42, 32, 44, 361, 9, 3860, 2358...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  [70, 746, 825, 109, 2083, 0, 2, 0, 0, 0, 9, 0,...      1\n",
       "1  [1209, 179, 1952, 4, 4959, 7, 0, 2, 978, 1522,...      1\n",
       "2  [287, 3, 3330, 0, 23, 12, 13, 465, 74, 8, 0, 8...      1\n",
       "3  [0, 0, 3, 592, 19, 2, 706, 1439, 2575, 7, 2, 0...      1\n",
       "4  [9, 2, 110, 12, 42, 32, 44, 361, 9, 3860, 2358...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d984313",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_all_d1 = d1[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56abf900",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_all_d1 = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75571eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts_all_d1:\n",
    "    for word in text:\n",
    "        counter_all_d1[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8f9adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4926"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_all_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1febe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4836"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_human_d1 = d1[d1[\"label\"] == 1][\"text\"]\n",
    "counter_human_d1 = Counter()\n",
    "for text in texts_human_d1:\n",
    "    for word in text:\n",
    "        counter_human_d1[word] +=1\n",
    "len(counter_human_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7dc56cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[\"text\"].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "018d4284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d2[\"text\"].apply(len) > 512).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6a541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.281128205128205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_human_d1.apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f399adaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4514"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_ai_d1 = d1[d1[\"label\"] == 0][\"text\"]\n",
    "counter_ai_d1 = Counter()\n",
    "for text in texts_ai_d1:\n",
    "    for word in text:\n",
    "        counter_ai_d1[word] +=1\n",
    "len(counter_ai_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd604db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.249435897435895"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_ai_d1.apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e087fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_json(path_or_buf=\"./data/domain2_train.json/domain2_train.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d337cced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4955"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_all_d2 = d2[\"text\"]\n",
    "counter_all_d2 = Counter()\n",
    "for text in texts_all_d2:\n",
    "    for word in text:\n",
    "        counter_all_d2[word] +=1\n",
    "len(counter_all_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f8069c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4914"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_human_d2 = d2[d2[\"label\"] == 1][\"text\"]\n",
    "counter_human_d2 = Counter()\n",
    "for text in texts_human_d2:\n",
    "    for word in text:\n",
    "        counter_human_d2[word] +=1\n",
    "len(counter_human_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b94a926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_human_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc3fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.9660465116279"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_human_d2.apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d1faf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_human_d2.apply(len).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad2fc978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4947"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_ai_d2 = d2[d2[\"label\"] == 0][\"text\"]\n",
    "counter_ai_d2 = Counter()\n",
    "for text in texts_ai_d2:\n",
    "    for word in text:\n",
    "        counter_ai_d2[word] +=1\n",
    "len(counter_ai_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fe0521c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146.33654901960784"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_ai_d2.apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d944a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12750"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_ai_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c8686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f15ef6cbd40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*counter_all_d1.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674798a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919cf637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 89837),\n",
       " (2, 56310),\n",
       " (1, 39104),\n",
       " (3, 39040),\n",
       " (6, 26353),\n",
       " (4, 19118),\n",
       " (5, 15303),\n",
       " (7, 14812),\n",
       " (9, 14685),\n",
       " (12, 10773),\n",
       " (17, 7948),\n",
       " (16, 7773),\n",
       " (48, 6090),\n",
       " (10, 5512),\n",
       " (8, 5432),\n",
       " (18, 5398),\n",
       " (15, 5273),\n",
       " (33, 4907),\n",
       " (13, 4746),\n",
       " (28, 4632),\n",
       " (22, 4338),\n",
       " (19, 4267),\n",
       " (62, 3844),\n",
       " (63, 3837),\n",
       " (30, 3832),\n",
       " (31, 3661),\n",
       " (47, 3394),\n",
       " (40, 2975),\n",
       " (24, 2794),\n",
       " (41, 2756),\n",
       " (35, 2686),\n",
       " (32, 2277),\n",
       " (38, 2270),\n",
       " (73, 2240),\n",
       " (112, 2170),\n",
       " (21, 2128),\n",
       " (103, 2109),\n",
       " (126, 2101),\n",
       " (96, 2028),\n",
       " (14, 1983),\n",
       " (104, 1945),\n",
       " (26, 1908),\n",
       " (29, 1766),\n",
       " (82, 1741),\n",
       " (95, 1741),\n",
       " (44, 1647),\n",
       " (42, 1625),\n",
       " (55, 1518),\n",
       " (43, 1514),\n",
       " (49, 1499),\n",
       " (51, 1479),\n",
       " (23, 1468),\n",
       " (142, 1317),\n",
       " (183, 1310),\n",
       " (211, 1302),\n",
       " (141, 1292),\n",
       " (213, 1270),\n",
       " (34, 1267),\n",
       " (189, 1257),\n",
       " (224, 1230),\n",
       " (206, 1229),\n",
       " (60, 1226),\n",
       " (64, 1208),\n",
       " (87, 1200),\n",
       " (70, 1153),\n",
       " (107, 1141),\n",
       " (80, 1136),\n",
       " (72, 1132),\n",
       " (11, 1129),\n",
       " (199, 1107),\n",
       " (89, 1105),\n",
       " (20, 1091),\n",
       " (45, 1056),\n",
       " (53, 1048),\n",
       " (56, 1016),\n",
       " (150, 1013),\n",
       " (74, 956),\n",
       " (93, 926),\n",
       " (50, 919),\n",
       " (281, 899),\n",
       " (61, 866),\n",
       " (217, 866),\n",
       " (312, 845),\n",
       " (59, 837),\n",
       " (118, 830),\n",
       " (255, 830),\n",
       " (136, 811),\n",
       " (353, 811),\n",
       " (92, 796),\n",
       " (132, 794),\n",
       " (36, 788),\n",
       " (195, 783),\n",
       " (77, 778),\n",
       " (351, 777),\n",
       " (84, 767),\n",
       " (283, 749),\n",
       " (108, 743),\n",
       " (52, 742),\n",
       " (119, 742),\n",
       " (407, 727),\n",
       " (233, 722),\n",
       " (411, 720),\n",
       " (78, 718),\n",
       " (68, 702),\n",
       " (426, 692),\n",
       " (113, 682),\n",
       " (69, 675),\n",
       " (152, 664),\n",
       " (66, 661),\n",
       " (71, 653),\n",
       " (54, 629),\n",
       " (58, 624),\n",
       " (425, 624),\n",
       " (100, 623),\n",
       " (165, 619),\n",
       " (393, 605),\n",
       " (86, 601),\n",
       " (170, 597),\n",
       " (67, 589),\n",
       " (65, 583),\n",
       " (355, 583),\n",
       " (157, 581),\n",
       " (294, 581),\n",
       " (266, 580),\n",
       " (334, 579),\n",
       " (90, 576),\n",
       " (295, 567),\n",
       " (394, 566),\n",
       " (179, 553),\n",
       " (247, 552),\n",
       " (210, 545),\n",
       " (504, 543),\n",
       " (192, 541),\n",
       " (27, 539),\n",
       " (128, 536),\n",
       " (81, 533),\n",
       " (167, 517),\n",
       " (542, 517),\n",
       " (528, 516),\n",
       " (135, 514),\n",
       " (485, 513),\n",
       " (242, 513),\n",
       " (462, 513),\n",
       " (122, 512),\n",
       " (130, 512),\n",
       " (102, 502),\n",
       " (97, 497),\n",
       " (263, 490),\n",
       " (205, 482),\n",
       " (259, 481),\n",
       " (538, 478),\n",
       " (139, 475),\n",
       " (498, 472),\n",
       " (285, 472),\n",
       " (593, 463),\n",
       " (120, 456),\n",
       " (442, 456),\n",
       " (360, 454),\n",
       " (269, 451),\n",
       " (408, 450),\n",
       " (379, 443),\n",
       " (234, 442),\n",
       " (654, 441),\n",
       " (185, 434),\n",
       " (110, 431),\n",
       " (223, 431),\n",
       " (109, 429),\n",
       " (146, 429),\n",
       " (140, 429),\n",
       " (390, 425),\n",
       " (587, 422),\n",
       " (137, 420),\n",
       " (287, 416),\n",
       " (382, 416),\n",
       " (687, 414),\n",
       " (714, 410),\n",
       " (550, 408),\n",
       " (202, 405),\n",
       " (509, 400),\n",
       " (79, 397),\n",
       " (164, 387),\n",
       " (488, 378),\n",
       " (161, 376),\n",
       " (777, 376),\n",
       " (257, 375),\n",
       " (691, 372),\n",
       " (473, 370),\n",
       " (280, 367),\n",
       " (201, 367),\n",
       " (117, 365),\n",
       " (85, 364),\n",
       " (759, 359),\n",
       " (156, 358),\n",
       " (571, 356),\n",
       " (692, 356),\n",
       " (548, 351),\n",
       " (841, 351),\n",
       " (482, 348),\n",
       " (252, 347),\n",
       " (177, 347),\n",
       " (75, 345),\n",
       " (698, 345),\n",
       " (771, 343),\n",
       " (648, 342),\n",
       " (803, 332),\n",
       " (718, 329),\n",
       " (752, 329),\n",
       " (262, 328),\n",
       " (292, 327),\n",
       " (346, 327),\n",
       " (344, 325),\n",
       " (480, 324),\n",
       " (153, 323),\n",
       " (124, 322),\n",
       " (169, 322),\n",
       " (37, 321),\n",
       " (545, 321),\n",
       " (500, 319),\n",
       " (562, 316),\n",
       " (761, 316),\n",
       " (91, 315),\n",
       " (543, 312),\n",
       " (317, 311),\n",
       " (405, 310),\n",
       " (647, 309),\n",
       " (903, 307),\n",
       " (160, 304),\n",
       " (908, 303),\n",
       " (159, 302),\n",
       " (254, 300),\n",
       " (933, 298),\n",
       " (767, 298),\n",
       " (730, 297),\n",
       " (862, 294),\n",
       " (443, 294),\n",
       " (239, 292),\n",
       " (219, 291),\n",
       " (143, 291),\n",
       " (148, 290),\n",
       " (736, 290),\n",
       " (495, 290),\n",
       " (250, 287),\n",
       " (613, 284),\n",
       " (732, 283),\n",
       " (39, 282),\n",
       " (998, 282),\n",
       " (175, 280),\n",
       " (230, 279),\n",
       " (907, 279),\n",
       " (627, 278),\n",
       " (1103, 277),\n",
       " (94, 276),\n",
       " (88, 273),\n",
       " (319, 273),\n",
       " (684, 273),\n",
       " (842, 273),\n",
       " (923, 273),\n",
       " (138, 272),\n",
       " (174, 272),\n",
       " (493, 272),\n",
       " (742, 272),\n",
       " (388, 272),\n",
       " (1003, 270),\n",
       " (147, 267),\n",
       " (106, 266),\n",
       " (579, 265),\n",
       " (1072, 265),\n",
       " (114, 261),\n",
       " (665, 261),\n",
       " (778, 258),\n",
       " (151, 258),\n",
       " (1163, 258),\n",
       " (1164, 257),\n",
       " (719, 256),\n",
       " (985, 255),\n",
       " (723, 254),\n",
       " (272, 254),\n",
       " (637, 254),\n",
       " (410, 252),\n",
       " (129, 252),\n",
       " (1126, 251),\n",
       " (1021, 250),\n",
       " (456, 248),\n",
       " (173, 245),\n",
       " (837, 242),\n",
       " (1151, 239),\n",
       " (897, 238),\n",
       " (279, 236),\n",
       " (1125, 236),\n",
       " (304, 234),\n",
       " (556, 234),\n",
       " (672, 233),\n",
       " (178, 232),\n",
       " (572, 232),\n",
       " (660, 232),\n",
       " (584, 230),\n",
       " (172, 229),\n",
       " (198, 229),\n",
       " (1057, 229),\n",
       " (215, 227),\n",
       " (489, 227),\n",
       " (522, 227),\n",
       " (1336, 227),\n",
       " (1342, 227),\n",
       " (544, 225),\n",
       " (766, 225),\n",
       " (1101, 225),\n",
       " (515, 224),\n",
       " (554, 223),\n",
       " (811, 218),\n",
       " (196, 218),\n",
       " (359, 218),\n",
       " (1312, 218),\n",
       " (127, 217),\n",
       " (207, 216),\n",
       " (1204, 216),\n",
       " (302, 214),\n",
       " (326, 213),\n",
       " (1129, 213),\n",
       " (286, 212),\n",
       " (925, 211),\n",
       " (454, 211),\n",
       " (214, 211),\n",
       " (552, 211),\n",
       " (134, 211),\n",
       " (271, 210),\n",
       " (98, 210),\n",
       " (378, 210),\n",
       " (850, 209),\n",
       " (624, 208),\n",
       " (216, 208),\n",
       " (744, 208),\n",
       " (1254, 207),\n",
       " (46, 205),\n",
       " (446, 204),\n",
       " (991, 204),\n",
       " (1451, 204),\n",
       " (711, 203),\n",
       " (1475, 203),\n",
       " (401, 202),\n",
       " (176, 202),\n",
       " (995, 202),\n",
       " (249, 201),\n",
       " (682, 201),\n",
       " (222, 201),\n",
       " (1506, 201),\n",
       " (166, 200),\n",
       " (604, 200),\n",
       " (626, 200),\n",
       " (251, 199),\n",
       " (1407, 199),\n",
       " (387, 198),\n",
       " (807, 197),\n",
       " (1321, 196),\n",
       " (869, 196),\n",
       " (221, 195),\n",
       " (299, 195),\n",
       " (1473, 195),\n",
       " (413, 194),\n",
       " (1563, 193),\n",
       " (1559, 193),\n",
       " (155, 192),\n",
       " (264, 192),\n",
       " (1287, 192),\n",
       " (440, 191),\n",
       " (1216, 191),\n",
       " (1054, 190),\n",
       " (315, 189),\n",
       " (783, 188),\n",
       " (1084, 188),\n",
       " (537, 187),\n",
       " (194, 186),\n",
       " (1531, 186),\n",
       " (181, 185),\n",
       " (499, 185),\n",
       " (349, 184),\n",
       " (356, 184),\n",
       " (209, 184),\n",
       " (145, 184),\n",
       " (814, 184),\n",
       " (227, 184),\n",
       " (643, 184),\n",
       " (792, 183),\n",
       " (494, 183),\n",
       " (1631, 183),\n",
       " (324, 182),\n",
       " (267, 182),\n",
       " (549, 182),\n",
       " (520, 182),\n",
       " (881, 182),\n",
       " (455, 182),\n",
       " (468, 182),\n",
       " (144, 181),\n",
       " (1231, 181),\n",
       " (398, 180),\n",
       " (306, 180),\n",
       " (193, 180),\n",
       " (1582, 179),\n",
       " (1085, 179),\n",
       " (645, 179),\n",
       " (708, 178),\n",
       " (277, 178),\n",
       " (1680, 178),\n",
       " (640, 177),\n",
       " (1077, 177),\n",
       " (535, 177),\n",
       " (1275, 177),\n",
       " (121, 176),\n",
       " (434, 176),\n",
       " (656, 176),\n",
       " (1091, 176),\n",
       " (1572, 176),\n",
       " (1642, 176),\n",
       " (1067, 175),\n",
       " (57, 175),\n",
       " (479, 174),\n",
       " (83, 174),\n",
       " (740, 173),\n",
       " (180, 173),\n",
       " (1305, 173),\n",
       " (810, 172),\n",
       " (1349, 172),\n",
       " (273, 172),\n",
       " (993, 172),\n",
       " (1479, 172),\n",
       " (298, 171),\n",
       " (1749, 171),\n",
       " (553, 170),\n",
       " (1256, 170),\n",
       " (260, 169),\n",
       " (1432, 169),\n",
       " (320, 169),\n",
       " (200, 167),\n",
       " (1286, 167),\n",
       " (724, 167),\n",
       " (797, 165),\n",
       " (244, 165),\n",
       " (381, 165),\n",
       " (368, 165),\n",
       " (511, 164),\n",
       " (472, 164),\n",
       " (769, 164),\n",
       " (1386, 164),\n",
       " (532, 163),\n",
       " (1404, 163),\n",
       " (847, 162),\n",
       " (314, 162),\n",
       " (617, 162),\n",
       " (1119, 161),\n",
       " (229, 161),\n",
       " (917, 161),\n",
       " (1820, 161),\n",
       " (101, 160),\n",
       " (330, 160),\n",
       " (855, 160),\n",
       " (1830, 160),\n",
       " (470, 159),\n",
       " (125, 159),\n",
       " (728, 159),\n",
       " (303, 158),\n",
       " (1005, 158),\n",
       " (162, 158),\n",
       " (671, 157),\n",
       " (1041, 156),\n",
       " (681, 156),\n",
       " (212, 156),\n",
       " (697, 156),\n",
       " (1273, 156),\n",
       " (116, 154),\n",
       " (354, 154),\n",
       " (1425, 154),\n",
       " (1369, 154),\n",
       " (1907, 154),\n",
       " (856, 153),\n",
       " (188, 153),\n",
       " (1186, 152),\n",
       " (1033, 152),\n",
       " (920, 151),\n",
       " (567, 150),\n",
       " (525, 150),\n",
       " (1423, 150),\n",
       " (1614, 149),\n",
       " (1360, 149),\n",
       " (1185, 149),\n",
       " (1604, 148),\n",
       " (1537, 147),\n",
       " (307, 147),\n",
       " (927, 147),\n",
       " (1647, 147),\n",
       " (406, 146),\n",
       " (1385, 146),\n",
       " (171, 146),\n",
       " (1142, 146),\n",
       " (557, 145),\n",
       " (403, 145),\n",
       " (614, 145),\n",
       " (469, 145),\n",
       " (433, 145),\n",
       " (636, 145),\n",
       " (835, 145),\n",
       " (395, 144),\n",
       " (384, 144),\n",
       " (695, 144),\n",
       " (282, 144),\n",
       " (603, 144),\n",
       " (1401, 144),\n",
       " (789, 144),\n",
       " (1725, 144),\n",
       " (1440, 144),\n",
       " (1599, 144),\n",
       " (203, 143),\n",
       " (288, 143),\n",
       " (1767, 143),\n",
       " (1323, 142),\n",
       " (685, 142),\n",
       " (1296, 142),\n",
       " (1315, 142),\n",
       " (1056, 141),\n",
       " (521, 141),\n",
       " (874, 141),\n",
       " (1690, 141),\n",
       " (753, 141),\n",
       " (812, 141),\n",
       " (347, 141),\n",
       " (412, 141),\n",
       " (1888, 141),\n",
       " (197, 140),\n",
       " (422, 140),\n",
       " (333, 140),\n",
       " (707, 140),\n",
       " (186, 140),\n",
       " (1436, 140),\n",
       " (1071, 139),\n",
       " (1239, 139),\n",
       " (467, 138),\n",
       " (1026, 138),\n",
       " (190, 138),\n",
       " (829, 138),\n",
       " (755, 138),\n",
       " (477, 137),\n",
       " (502, 137),\n",
       " (1238, 137),\n",
       " (782, 136),\n",
       " (340, 136),\n",
       " (674, 136),\n",
       " (1020, 136),\n",
       " (1743, 136),\n",
       " (417, 135),\n",
       " (775, 135),\n",
       " (944, 135),\n",
       " (449, 135),\n",
       " (338, 134),\n",
       " (650, 134),\n",
       " (1494, 134),\n",
       " (1409, 134),\n",
       " (348, 134),\n",
       " (1691, 134),\n",
       " (1713, 134),\n",
       " (430, 133),\n",
       " (329, 133),\n",
       " (1161, 133),\n",
       " (1141, 133),\n",
       " (1674, 133),\n",
       " (1118, 133),\n",
       " (376, 133),\n",
       " (678, 132),\n",
       " (1389, 132),\n",
       " (596, 132),\n",
       " (949, 131),\n",
       " (839, 131),\n",
       " (474, 131),\n",
       " (2008, 131),\n",
       " (641, 130),\n",
       " (380, 130),\n",
       " (605, 130),\n",
       " (1754, 130),\n",
       " (2174, 130),\n",
       " (204, 129),\n",
       " (366, 129),\n",
       " (845, 129),\n",
       " (131, 129),\n",
       " (1864, 129),\n",
       " (510, 129),\n",
       " (2116, 129),\n",
       " (248, 128),\n",
       " (2005, 127),\n",
       " (1672, 127),\n",
       " (1763, 127),\n",
       " (667, 127),\n",
       " (961, 127),\n",
       " (1047, 127),\n",
       " (895, 127),\n",
       " (1963, 127),\n",
       " (226, 126),\n",
       " (458, 126),\n",
       " (253, 126),\n",
       " (747, 126),\n",
       " (2081, 126),\n",
       " (1643, 126),\n",
       " (1598, 126),\n",
       " (892, 125),\n",
       " (400, 125),\n",
       " (954, 125),\n",
       " (1498, 125),\n",
       " (1661, 125),\n",
       " (1190, 124),\n",
       " (1147, 124),\n",
       " (1464, 123),\n",
       " (836, 123),\n",
       " (602, 123),\n",
       " (1123, 123),\n",
       " (644, 123),\n",
       " (1651, 123),\n",
       " (1076, 122),\n",
       " (1198, 122),\n",
       " (868, 122),\n",
       " (321, 122),\n",
       " (444, 122),\n",
       " (819, 122),\n",
       " (225, 122),\n",
       " (781, 122),\n",
       " (746, 121),\n",
       " (896, 121),\n",
       " (218, 121),\n",
       " (824, 121),\n",
       " (1605, 121),\n",
       " (729, 121),\n",
       " (1018, 121),\n",
       " (2281, 121),\n",
       " (484, 120),\n",
       " (428, 120),\n",
       " (335, 120),\n",
       " (770, 119),\n",
       " (1017, 119),\n",
       " (628, 119),\n",
       " (2145, 119),\n",
       " (2330, 119),\n",
       " (2331, 119),\n",
       " (1243, 118),\n",
       " (231, 118),\n",
       " (1514, 118),\n",
       " (1924, 118),\n",
       " (1776, 118),\n",
       " (2344, 118),\n",
       " (1809, 117),\n",
       " (232, 117),\n",
       " (105, 117),\n",
       " (857, 117),\n",
       " (1188, 117),\n",
       " (375, 117),\n",
       " (915, 117),\n",
       " (866, 117),\n",
       " (2335, 117),\n",
       " (809, 116),\n",
       " (716, 116),\n",
       " (531, 116),\n",
       " (863, 116),\n",
       " (115, 116),\n",
       " (184, 116),\n",
       " (801, 116),\n",
       " (1050, 116),\n",
       " (825, 115),\n",
       " (445, 115),\n",
       " (606, 115),\n",
       " (784, 114),\n",
       " (585, 114),\n",
       " (513, 114),\n",
       " (634, 114),\n",
       " (909, 114),\n",
       " (1969, 114),\n",
       " (900, 113),\n",
       " (2064, 113),\n",
       " (1045, 113),\n",
       " (865, 113),\n",
       " (1806, 113),\n",
       " (1083, 113),\n",
       " (1931, 113),\n",
       " (2454, 113),\n",
       " (2469, 111),\n",
       " (898, 111),\n",
       " (343, 111),\n",
       " (431, 111),\n",
       " (864, 111),\n",
       " (2227, 111),\n",
       " (2481, 111),\n",
       " (1359, 110),\n",
       " (370, 110),\n",
       " (651, 110),\n",
       " (802, 110),\n",
       " (854, 110),\n",
       " (308, 110),\n",
       " (1264, 110),\n",
       " (1240, 110),\n",
       " (1701, 110),\n",
       " (2340, 110),\n",
       " (276, 109),\n",
       " (1267, 109),\n",
       " (953, 109),\n",
       " (1904, 109),\n",
       " (1014, 108),\n",
       " (2384, 108),\n",
       " (332, 108),\n",
       " (1967, 108),\n",
       " (2447, 108),\n",
       " (2491, 108),\n",
       " (358, 107),\n",
       " (947, 107),\n",
       " (622, 107),\n",
       " (1262, 107),\n",
       " (586, 107),\n",
       " (1517, 107),\n",
       " (246, 107),\n",
       " (2433, 107),\n",
       " (592, 106),\n",
       " (361, 106),\n",
       " (236, 106),\n",
       " (851, 106),\n",
       " (1918, 106),\n",
       " (2536, 106),\n",
       " (1709, 106),\n",
       " (1090, 106),\n",
       " (316, 106),\n",
       " (642, 106),\n",
       " (984, 106),\n",
       " (424, 106),\n",
       " (1002, 106),\n",
       " (822, 106),\n",
       " (1295, 106),\n",
       " (2179, 106),\n",
       " (1524, 105),\n",
       " (1452, 105),\n",
       " (1298, 105),\n",
       " (816, 105),\n",
       " (616, 104),\n",
       " (1234, 104),\n",
       " (722, 104),\n",
       " (318, 104),\n",
       " (323, 104),\n",
       " (514, 104),\n",
       " (1011, 104),\n",
       " (133, 104),\n",
       " (1843, 104),\n",
       " (1480, 104),\n",
       " (1987, 104),\n",
       " (955, 103),\n",
       " (973, 103),\n",
       " (163, 103),\n",
       " (2181, 103),\n",
       " (2313, 103),\n",
       " (608, 103),\n",
       " (399, 103),\n",
       " (1801, 103),\n",
       " (1250, 103),\n",
       " (962, 103),\n",
       " (275, 102),\n",
       " (580, 102),\n",
       " (492, 102),\n",
       " (701, 102),\n",
       " (1348, 102),\n",
       " (496, 102),\n",
       " (464, 102),\n",
       " (2343, 102),\n",
       " (1567, 102),\n",
       " (2242, 102),\n",
       " (2349, 102),\n",
       " (415, 101),\n",
       " (297, 101),\n",
       " (1200, 101),\n",
       " (383, 101),\n",
       " (1901, 101),\n",
       " (339, 101),\n",
       " (1609, 101),\n",
       " (357, 101),\n",
       " (1508, 101),\n",
       " (2046, 101),\n",
       " (457, 100),\n",
       " (256, 100),\n",
       " (1792, 100),\n",
       " (1217, 100),\n",
       " (632, 100),\n",
       " (853, 100),\n",
       " (2026, 100),\n",
       " (2080, 100),\n",
       " (706, 99),\n",
       " (994, 99),\n",
       " (350, 99),\n",
       " (670, 99),\n",
       " (1600, 99),\n",
       " (385, 99),\n",
       " (815, 99),\n",
       " (1092, 99),\n",
       " (1941, 99),\n",
       " (1376, 99),\n",
       " (1819, 99),\n",
       " (2117, 99),\n",
       " (2382, 99),\n",
       " (1133, 98),\n",
       " (1039, 98),\n",
       " (696, 98),\n",
       " (731, 98),\n",
       " (1773, 98),\n",
       " (1466, 98),\n",
       " (2261, 98),\n",
       " (2010, 98),\n",
       " (919, 98),\n",
       " (1445, 98),\n",
       " (2662, 98),\n",
       " (2719, 98),\n",
       " (191, 97),\n",
       " (76, 97),\n",
       " (2657, 97),\n",
       " (1579, 96),\n",
       " (486, 96),\n",
       " (1205, 96),\n",
       " (402, 96),\n",
       " (565, 96),\n",
       " (471, 96),\n",
       " (1189, 96),\n",
       " (389, 95),\n",
       " (969, 95),\n",
       " (1131, 95),\n",
       " (1747, 95),\n",
       " (501, 95),\n",
       " (1311, 95),\n",
       " (432, 95),\n",
       " (1958, 95),\n",
       " (2624, 95),\n",
       " (182, 94),\n",
       " (325, 94),\n",
       " (726, 94),\n",
       " (2142, 94),\n",
       " (2140, 94),\n",
       " (720, 93),\n",
       " (936, 93),\n",
       " (371, 93),\n",
       " (284, 93),\n",
       " (738, 93),\n",
       " (960, 93),\n",
       " (941, 93),\n",
       " (1799, 93),\n",
       " (2770, 93),\n",
       " (1935, 93),\n",
       " (2565, 93),\n",
       " (1007, 92),\n",
       " (341, 92),\n",
       " (301, 92),\n",
       " (1168, 92),\n",
       " (657, 92),\n",
       " (2508, 92),\n",
       " (1116, 92),\n",
       " (2862, 92),\n",
       " (369, 91),\n",
       " (2329, 91),\n",
       " (680, 91),\n",
       " (2459, 91),\n",
       " (1532, 91),\n",
       " (1044, 91),\n",
       " (600, 91),\n",
       " (1196, 91),\n",
       " (2323, 91),\n",
       " (2161, 91),\n",
       " (1818, 91),\n",
       " (831, 91),\n",
       " (2888, 91),\n",
       " (1684, 90),\n",
       " (365, 90),\n",
       " (1770, 90),\n",
       " (436, 90),\n",
       " (989, 90),\n",
       " (1491, 90),\n",
       " (1160, 90),\n",
       " (487, 90),\n",
       " (1022, 90),\n",
       " (2266, 90),\n",
       " (1666, 90),\n",
       " (1891, 90),\n",
       " (1418, 90),\n",
       " (1873, 90),\n",
       " (261, 90),\n",
       " (1758, 90),\n",
       " (2057, 90),\n",
       " (2564, 90),\n",
       " (2042, 89),\n",
       " (481, 89),\n",
       " (397, 89),\n",
       " (265, 89),\n",
       " (887, 89),\n",
       " (633, 89),\n",
       " (2342, 89),\n",
       " (1779, 89),\n",
       " (2107, 89),\n",
       " (2289, 89),\n",
       " (1368, 89),\n",
       " (971, 88),\n",
       " (1073, 88),\n",
       " (420, 88),\n",
       " (1571, 88),\n",
       " (517, 88),\n",
       " (1412, 88),\n",
       " (704, 88),\n",
       " (1564, 88),\n",
       " (1290, 88),\n",
       " (1662, 88),\n",
       " (491, 88),\n",
       " (785, 88),\n",
       " (441, 88),\n",
       " (662, 88),\n",
       " (2626, 88),\n",
       " (1919, 88),\n",
       " (2394, 88),\n",
       " (1157, 88),\n",
       " (2755, 88),\n",
       " (2363, 88),\n",
       " (2513, 88),\n",
       " (1302, 87),\n",
       " (1523, 87),\n",
       " (497, 87),\n",
       " (833, 87),\n",
       " (804, 87),\n",
       " (1324, 87),\n",
       " (1430, 87),\n",
       " (2038, 87),\n",
       " (3019, 87),\n",
       " (939, 86),\n",
       " (1322, 86),\n",
       " (966, 86),\n",
       " (610, 86),\n",
       " (1313, 86),\n",
       " (930, 86),\n",
       " (1718, 86),\n",
       " (1167, 86),\n",
       " (1408, 86),\n",
       " (838, 86),\n",
       " (573, 86),\n",
       " (290, 86),\n",
       " (700, 86),\n",
       " (1138, 86),\n",
       " (452, 86),\n",
       " (1513, 86),\n",
       " (3041, 86),\n",
       " (2967, 86),\n",
       " (1529, 85),\n",
       " (2169, 85),\n",
       " (2067, 85),\n",
       " (1742, 85),\n",
       " (2833, 85),\n",
       " (2176, 85),\n",
       " (3075, 85),\n",
       " (2119, 84),\n",
       " (808, 84),\n",
       " (529, 84),\n",
       " (1415, 84),\n",
       " (1134, 84),\n",
       " (823, 84),\n",
       " (1558, 84),\n",
       " (1700, 84),\n",
       " (1910, 84),\n",
       " (2112, 84),\n",
       " (1344, 84),\n",
       " (270, 83),\n",
       " (1367, 83),\n",
       " (620, 83),\n",
       " (1640, 83),\n",
       " (717, 83),\n",
       " (2147, 83),\n",
       " (363, 83),\n",
       " (1233, 83),\n",
       " (2630, 83),\n",
       " (832, 83),\n",
       " (2717, 83),\n",
       " (362, 82),\n",
       " (505, 82),\n",
       " (478, 82),\n",
       " (439, 82),\n",
       " (1197, 82),\n",
       " (757, 82),\n",
       " (311, 82),\n",
       " (1756, 82),\n",
       " (2404, 82),\n",
       " (372, 81),\n",
       " (1064, 81),\n",
       " (1049, 81),\n",
       " (980, 81),\n",
       " (2095, 81),\n",
       " (1175, 81),\n",
       " (2806, 81),\n",
       " (1089, 81),\n",
       " (677, 81),\n",
       " (2301, 81),\n",
       " (1889, 81),\n",
       " (3090, 81),\n",
       " (1304, 81),\n",
       " (2910, 81),\n",
       " (1004, 80),\n",
       " (773, 80),\n",
       " (289, 80),\n",
       " (2115, 80),\n",
       " (669, 80),\n",
       " (683, 80),\n",
       " (1618, 80),\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_all_d1.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdd322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a866f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testList2_human = [log(elem2) for elem1, elem2 in counter_human_d1.most_common()]\n",
    "testList2_ai = [log(elem2) for elem1, elem2 in counter_ai_d1.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "793ba61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.693489174459955,\n",
       " 9.602112152952344,\n",
       " 9.352534137679358,\n",
       " 8.952864141581468,\n",
       " 8.826000545482966,\n",
       " 8.75115798136203,\n",
       " 8.617038526385954,\n",
       " 8.61323037961318,\n",
       " 8.466320861042481,\n",
       " 7.867105500316739,\n",
       " 7.8050670442584895,\n",
       " 7.742835955430749,\n",
       " 7.62657020629066,\n",
       " 7.528869256642251,\n",
       " 7.441906728051625,\n",
       " 7.299797366758161,\n",
       " 7.27931883541462,\n",
       " 7.25063551189868,\n",
       " 7.135687347028144,\n",
       " 7.083387847625295,\n",
       " 7.074963197966044,\n",
       " 7.07326971745971,\n",
       " 7.023758954738443,\n",
       " 6.956545443151569,\n",
       " 6.902742737158593,\n",
       " 6.897704943128636,\n",
       " 6.874198495453294,\n",
       " 6.872128101338986,\n",
       " 6.8690144506657065,\n",
       " 6.851184927493743,\n",
       " 6.838405200847344,\n",
       " 6.822197390620491,\n",
       " 6.816735880594968,\n",
       " 6.787844982309579,\n",
       " 6.760414691083428,\n",
       " 6.741700694652055,\n",
       " 6.635946555686647,\n",
       " 6.61472560020376,\n",
       " 6.612041034833092,\n",
       " 6.513230110912307,\n",
       " 6.455198563340122,\n",
       " 6.450470422144176,\n",
       " 6.450470422144176,\n",
       " 6.403574197934815,\n",
       " 6.400257445308821,\n",
       " 6.3818160174060985,\n",
       " 6.315358001522335,\n",
       " 6.313548046277095,\n",
       " 6.298949246855942,\n",
       " 6.278521424165844,\n",
       " 6.263398262591624,\n",
       " 6.255750041753367,\n",
       " 6.236369590203704,\n",
       " 6.208590026096629,\n",
       " 6.1779441140506,\n",
       " 6.171700597410915,\n",
       " 6.142037405587356,\n",
       " 6.139884552226255,\n",
       " 6.126869184114185,\n",
       " 6.1224928095143865,\n",
       " 6.115892125483034,\n",
       " 6.095824562432225,\n",
       " 6.068425588244111,\n",
       " 5.973809611869261,\n",
       " 5.955837369464831,\n",
       " 5.8971538676367405,\n",
       " 5.8888779583328805,\n",
       " 5.872117789475416,\n",
       " 5.860786223465865,\n",
       " 5.849324779946859,\n",
       " 5.840641657373398,\n",
       " 5.8289456176102075,\n",
       " 5.811140992976701,\n",
       " 5.793013608384144,\n",
       " 5.7745515455444085,\n",
       " 5.768320995793772,\n",
       " 5.765191102784844,\n",
       " 5.75890177387728,\n",
       " 5.717027701406222,\n",
       " 5.71042701737487,\n",
       " 5.6937321388027,\n",
       " 5.69035945432406,\n",
       " 5.68697535633982,\n",
       " 5.680172609017068,\n",
       " 5.676753802268282,\n",
       " 5.659482215759621,\n",
       " 5.62040086571715,\n",
       " 5.6131281063880705,\n",
       " 5.5909869805108565,\n",
       " 5.58724865840025,\n",
       " 5.575949103146316,\n",
       " 5.568344503761097,\n",
       " 5.537334267018537,\n",
       " 5.529429087511423,\n",
       " 5.529429087511423,\n",
       " 5.5053315359323625,\n",
       " 5.497168225293202,\n",
       " 5.480638923341991,\n",
       " 5.4680601411351315,\n",
       " 5.4638318050256105,\n",
       " 5.459585514144159,\n",
       " 5.455321115357702,\n",
       " 5.407171771460119,\n",
       " 5.393627546352362,\n",
       " 5.389071729816501,\n",
       " 5.384495062789089,\n",
       " 5.384495062789089,\n",
       " 5.356586274672012,\n",
       " 5.342334251964811,\n",
       " 5.342334251964811,\n",
       " 5.332718793265369,\n",
       " 5.303304908059076,\n",
       " 5.298317366548036,\n",
       " 5.293304824724492,\n",
       " 5.278114659230517,\n",
       " 5.247024072160486,\n",
       " 5.247024072160486,\n",
       " 5.236441962829949,\n",
       " 5.236441962829949,\n",
       " 5.236441962829949,\n",
       " 5.236441962829949,\n",
       " 5.231108616854587,\n",
       " 5.231108616854587,\n",
       " 5.220355825078324,\n",
       " 5.214935757608986,\n",
       " 5.209486152841421,\n",
       " 5.209486152841421,\n",
       " 5.198497031265826,\n",
       " 5.198497031265826,\n",
       " 5.19295685089021,\n",
       " 5.181783550292085,\n",
       " 5.176149732573829,\n",
       " 5.170483995038151,\n",
       " 5.159055299214529,\n",
       " 5.159055299214529,\n",
       " 5.135798437050262,\n",
       " 5.123963979403259,\n",
       " 5.117993812416755,\n",
       " 5.111987788356544,\n",
       " 5.10594547390058,\n",
       " 5.10594547390058,\n",
       " 5.093750200806762,\n",
       " 5.087596335232384,\n",
       " 5.075173815233827,\n",
       " 5.075173815233827,\n",
       " 5.056245805348308,\n",
       " 5.043425116919247,\n",
       " 5.030437921392435,\n",
       " 5.030437921392435,\n",
       " 5.030437921392435,\n",
       " 5.0106352940962555,\n",
       " 5.003946305945459,\n",
       " 5.003946305945459,\n",
       " 4.997212273764115,\n",
       " 4.990432586778736,\n",
       " 4.948759890378168,\n",
       " 4.948759890378168,\n",
       " 4.948759890378168,\n",
       " 4.948759890378168,\n",
       " 4.948759890378168,\n",
       " 4.941642422609304,\n",
       " 4.927253685157205,\n",
       " 4.927253685157205,\n",
       " 4.912654885736052,\n",
       " 4.90527477843843,\n",
       " 4.890349128221754,\n",
       " 4.859812404361672,\n",
       " 4.852030263919617,\n",
       " 4.844187086458591,\n",
       " 4.836281906951478,\n",
       " 4.820281565605037,\n",
       " 4.812184355372417,\n",
       " 4.812184355372417,\n",
       " 4.812184355372417,\n",
       " 4.812184355372417,\n",
       " 4.804021044733257,\n",
       " 4.795790545596741,\n",
       " 4.787491742782046,\n",
       " 4.787491742782046,\n",
       " 4.77912349311153,\n",
       " 4.77912349311153,\n",
       " 4.77912349311153,\n",
       " 4.77912349311153,\n",
       " 4.762173934797756,\n",
       " 4.762173934797756,\n",
       " 4.762173934797756,\n",
       " 4.7535901911063645,\n",
       " 4.7535901911063645,\n",
       " 4.74493212836325,\n",
       " 4.74493212836325,\n",
       " 4.74493212836325,\n",
       " 4.736198448394496,\n",
       " 4.736198448394496,\n",
       " 4.727387818712341,\n",
       " 4.718498871295094,\n",
       " 4.718498871295094,\n",
       " 4.718498871295094,\n",
       " 4.718498871295094,\n",
       " 4.709530201312334,\n",
       " 4.709530201312334,\n",
       " 4.700480365792417,\n",
       " 4.6913478822291435,\n",
       " 4.6913478822291435,\n",
       " 4.672828834461906,\n",
       " 4.653960350157523,\n",
       " 4.653960350157523,\n",
       " 4.653960350157523,\n",
       " 4.653960350157523,\n",
       " 4.653960350157523,\n",
       " 4.6443908991413725,\n",
       " 4.6443908991413725,\n",
       " 4.6443908991413725,\n",
       " 4.6443908991413725,\n",
       " 4.634728988229636,\n",
       " 4.634728988229636,\n",
       " 4.634728988229636,\n",
       " 4.634728988229636,\n",
       " 4.624972813284271,\n",
       " 4.61512051684126,\n",
       " 4.61512051684126,\n",
       " 4.61512051684126,\n",
       " 4.605170185988092,\n",
       " 4.605170185988092,\n",
       " 4.605170185988092,\n",
       " 4.59511985013459,\n",
       " 4.59511985013459,\n",
       " 4.59511985013459,\n",
       " 4.584967478670572,\n",
       " 4.584967478670572,\n",
       " 4.574710978503383,\n",
       " 4.574710978503383,\n",
       " 4.574710978503383,\n",
       " 4.574710978503383,\n",
       " 4.574710978503383,\n",
       " 4.574710978503383,\n",
       " 4.564348191467836,\n",
       " 4.553876891600541,\n",
       " 4.553876891600541,\n",
       " 4.553876891600541,\n",
       " 4.543294782270004,\n",
       " 4.543294782270004,\n",
       " 4.543294782270004,\n",
       " 4.543294782270004,\n",
       " 4.543294782270004,\n",
       " 4.543294782270004,\n",
       " 4.543294782270004,\n",
       " 4.532599493153256,\n",
       " 4.5217885770490405,\n",
       " 4.51085950651685,\n",
       " 4.51085950651685,\n",
       " 4.51085950651685,\n",
       " 4.499809670330265,\n",
       " 4.499809670330265,\n",
       " 4.48863636973214,\n",
       " 4.48863636973214,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.465908118654584,\n",
       " 4.454347296253507,\n",
       " 4.454347296253507,\n",
       " 4.454347296253507,\n",
       " 4.454347296253507,\n",
       " 4.454347296253507,\n",
       " 4.442651256490317,\n",
       " 4.442651256490317,\n",
       " 4.442651256490317,\n",
       " 4.430816798843313,\n",
       " 4.430816798843313,\n",
       " 4.418840607796598,\n",
       " 4.418840607796598,\n",
       " 4.418840607796598,\n",
       " 4.418840607796598,\n",
       " 4.418840607796598,\n",
       " 4.418840607796598,\n",
       " 4.418840607796598,\n",
       " 4.406719247264253,\n",
       " 4.406719247264253,\n",
       " 4.406719247264253,\n",
       " 4.406719247264253,\n",
       " 4.406719247264253,\n",
       " 4.406719247264253,\n",
       " 4.406719247264253,\n",
       " 4.394449154672439,\n",
       " 4.394449154672439,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.382026634673881,\n",
       " 4.3694478524670215,\n",
       " 4.3694478524670215,\n",
       " 4.3694478524670215,\n",
       " 4.3694478524670215,\n",
       " 4.356708826689592,\n",
       " 4.356708826689592,\n",
       " 4.356708826689592,\n",
       " 4.356708826689592,\n",
       " 4.356708826689592,\n",
       " 4.356708826689592,\n",
       " 4.343805421853684,\n",
       " 4.343805421853684,\n",
       " 4.343805421853684,\n",
       " 4.343805421853684,\n",
       " 4.343805421853684,\n",
       " 4.343805421853684,\n",
       " 4.343805421853684,\n",
       " 4.330733340286331,\n",
       " 4.330733340286331,\n",
       " 4.330733340286331,\n",
       " 4.330733340286331,\n",
       " 4.330733340286331,\n",
       " 4.330733340286331,\n",
       " 4.330733340286331,\n",
       " 4.31748811353631,\n",
       " 4.31748811353631,\n",
       " 4.31748811353631,\n",
       " 4.31748811353631,\n",
       " 4.31748811353631,\n",
       " 4.31748811353631,\n",
       " 4.30406509320417,\n",
       " 4.30406509320417,\n",
       " 4.30406509320417,\n",
       " 4.30406509320417,\n",
       " 4.290459441148391,\n",
       " 4.290459441148391,\n",
       " 4.290459441148391,\n",
       " 4.290459441148391,\n",
       " 4.290459441148391,\n",
       " 4.276666119016055,\n",
       " 4.276666119016055,\n",
       " 4.276666119016055,\n",
       " 4.276666119016055,\n",
       " 4.276666119016055,\n",
       " 4.276666119016055,\n",
       " 4.276666119016055,\n",
       " 4.2626798770413155,\n",
       " 4.2626798770413155,\n",
       " 4.2626798770413155,\n",
       " 4.248495242049359,\n",
       " 4.248495242049359,\n",
       " 4.248495242049359,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.23410650459726,\n",
       " 4.219507705176107,\n",
       " 4.219507705176107,\n",
       " 4.219507705176107,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.204692619390966,\n",
       " 4.189654742026425,\n",
       " 4.189654742026425,\n",
       " 4.189654742026425,\n",
       " 4.189654742026425,\n",
       " 4.189654742026425,\n",
       " 4.189654742026425,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.174387269895637,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.1588830833596715,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.143134726391533,\n",
       " 4.127134385045092,\n",
       " 4.127134385045092,\n",
       " 4.127134385045092,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.110873864173311,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.0943445622221,\n",
       " 4.07753744390572,\n",
       " 4.07753744390572,\n",
       " 4.07753744390572,\n",
       " 4.07753744390572,\n",
       " 4.07753744390572,\n",
       " 4.07753744390572,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.060443010546419,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.04305126783455,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.02535169073515,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 4.007333185232471,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.9889840465642745,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.970291913552122,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9512437185814275,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.9318256327243257,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.912023005428146,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.8918202981106265,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.871201010907891,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.8501476017100584,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.828641396489095,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.8066624897703196,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.784189633918261,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7612001156935624,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.7376696182833684,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.713572066704308,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6888794541139363,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6635616461296463,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6375861597263857,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.6109179126442243,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.58351893845611,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5553480614894135,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.5263605246161616,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4965075614664802,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4657359027997265,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4339872044851463,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " 3.4011973816621555,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testList2_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a457a3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(testList2_human)\n",
    "\n",
    "plt.plot(testList2_ai)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f29737d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testList_human = [log(elem2*6) for elem1, elem2 in counter_human_d2.most_common()]\n",
    "testList_ai = [log(elem2) for elem1, elem2 in counter_ai_d2.most_common()]\n",
    "plt.plot(testList_human)\n",
    "\n",
    "plt.plot(testList_ai)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93f2afb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.930232558139535"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_ai_d2)/len(texts_human_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082b214",
   "metadata": {},
   "source": [
    "# Feature Engineering/Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "218f1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51afa852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BOW(row):\n",
    "    text = row['text']\n",
    "    bow = [0 for i in range(VOCAB_SIZE)]\n",
    "    for idx in text:\n",
    "        bow[idx]+=1\n",
    "    return np.array(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37d33d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary explicitly as a list of integers from 0 to 4999\n",
    "vocabulary = [str(i) for i in range(VOCAB_SIZE)]\n",
    "\n",
    "# Initialize the CountVectorizer with the predefined vocabulary\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Fit and transform the text data to obtain a feature matrix\n",
    "bow_matrix_1 = vectorizer.fit_transform(d1[\"text\"].apply(lambda x: \" \".join(map(str, x))))\n",
    "\n",
    "# Convert the feature matrix to a dense NumPy array if needed\n",
    "#dense_bow_matrix = bow_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d27eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_matrix_1, d1['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a4558c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15600x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 307552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5269ed08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18319    0\n",
       "8086     1\n",
       "447      1\n",
       "12545    0\n",
       "7176     1\n",
       "        ..\n",
       "11284    0\n",
       "11964    0\n",
       "5390     1\n",
       "860      1\n",
       "15795    0\n",
       "Name: label, Length: 15600, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74a95c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 5000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b492e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold(threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1afd25c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9076923076923077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90      1943\n",
      "           1       0.88      0.94      0.91      1957\n",
      "\n",
      "    accuracy                           0.91      3900\n",
      "   macro avg       0.91      0.91      0.91      3900\n",
      "weighted avg       0.91      0.91      0.91      3900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Train a logistic regression model.\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "040e1cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9073333333333334\n",
      "0.9097560012959486\n",
      "0.9633188165680474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(model, bow_matrix_1, d1['label'], cv=5,scoring=('balanced_accuracy', 'f1', 'roc_auc'))\n",
    "print(scores['test_balanced_accuracy'].mean())\n",
    "print(scores['test_f1'].mean())\n",
    "print(scores['test_roc_auc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85b48389",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vt_1 = vt.fit_transform(bow_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "031375a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 3177)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vt_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf3feb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9061538461538462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      1943\n",
      "           1       0.88      0.94      0.91      1957\n",
      "\n",
      "    accuracy                           0.91      3900\n",
      "   macro avg       0.91      0.91      0.91      3900\n",
      "weighted avg       0.91      0.91      0.91      3900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_vt_1, d1['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5ddb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81225f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_1 = tfidf.fit_transform(bow_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edaebbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9048717948717949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90      1943\n",
      "           1       0.86      0.97      0.91      1957\n",
      "\n",
      "    accuracy                           0.90      3900\n",
      "   macro avg       0.91      0.90      0.90      3900\n",
      "weighted avg       0.91      0.90      0.90      3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_t_train, X_t_test, y_t_train, y_t_test = train_test_split(tfidf_matrix_1, d1['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_t_train, y_t_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_t_pred = model.predict(X_t_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_t_test, y_t_pred)\n",
    "report = classification_report(y_t_test, y_t_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9becd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041538461538462\n",
      "0.9096091460034728\n",
      "0.9610606706114397\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(model, tfidf_matrix_1, d1['label'], cv=5,scoring=('balanced_accuracy', 'f1', 'roc_auc'))\n",
    "print(scores['test_balanced_accuracy'].mean())\n",
    "print(scores['test_f1'].mean())\n",
    "print(scores['test_roc_auc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f92461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt1 = VarianceThreshold(threshold=0.0001)\n",
    "tfidf_vt_1 = vt1.fit_transform(tfidf_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c901c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 2687)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vt_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8dd47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9012820512820513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.89      1943\n",
      "           1       0.86      0.96      0.91      1957\n",
      "\n",
      "    accuracy                           0.90      3900\n",
      "   macro avg       0.91      0.90      0.90      3900\n",
      "weighted avg       0.91      0.90      0.90      3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vt_1, d1['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afe95daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix_2 = vectorizer.fit_transform(d2[\"text\"].apply(lambda x: \" \".join(map(str, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d8ba784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8067114093959732\n",
      "f1: 0.20661157024793392\n",
      "roc_auc_score: 0.5904268363048872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89      2544\n",
      "           1       0.26      0.17      0.21       436\n",
      "\n",
      "    accuracy                           0.81      2980\n",
      "   macro avg       0.56      0.54      0.55      2980\n",
      "weighted avg       0.78      0.81      0.79      2980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "371612ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f583ce47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2329,  215],\n",
       "       [ 361,   75]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7acee041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_validate(model, bow_matrix_2, d2['label'], cv=5,scoring=('balanced_accuracy', 'f1', 'roc_auc'))\n",
    "# print(scores['test_balanced_accuracy'].mean())\n",
    "# print(scores['test_f1'].mean())\n",
    "# print(scores['test_roc_auc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b0e34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "611f3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oversample(X_train, y_train, X_test, y_test, oversample, model):\n",
    "    X_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n",
    "    model.fit(X_train_o, y_train_o)\n",
    "\n",
    "    # Predict on the test set.\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "    print(f\"roc_auc_score: {roc_auc}\")\n",
    "    print(report)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55148bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7345637583892617\n",
      "f1: 0.27762557077625577\n",
      "roc_auc_score: 0.572485719231435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84      2544\n",
      "           1       0.23      0.35      0.28       436\n",
      "\n",
      "    accuracy                           0.73      2980\n",
      "   macro avg       0.55      0.57      0.56      2980\n",
      "weighted avg       0.78      0.73      0.76      2980\n",
      "\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "test_oversample(X_train, y_train, X_test, y_test, oversample, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e90bd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7479865771812081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.85      2544\n",
      "           1       0.25      0.37      0.30       436\n",
      "\n",
      "    accuracy                           0.75      2980\n",
      "   macro avg       0.57      0.59      0.57      2980\n",
      "weighted avg       0.79      0.75      0.77      2980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n",
    "model.fit(X_train_o, y_train_o)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d22b5949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7550335570469798\n",
      "f1: 0.22008547008547008\n",
      "roc_auc_score: 0.5304881786971323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85      2544\n",
      "           1       0.21      0.24      0.22       436\n",
      "\n",
      "    accuracy                           0.76      2980\n",
      "   macro avg       0.54      0.54      0.54      2980\n",
      "weighted avg       0.77      0.76      0.76      2980\n",
      "\n",
      "----------------------------------------\n",
      "CPU times: user 425 ms, sys: 36.1 ms, total: 461 ms\n",
      "Wall time: 460 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "smote = SMOTE()\n",
    "test_oversample(X_train, y_train, X_test, y_test, smote, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5eb12ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7550335570469798\n",
      "f1: 0.2283298097251586\n",
      "roc_auc_score: 0.5347633936299117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85      2544\n",
      "           1       0.21      0.25      0.23       436\n",
      "\n",
      "    accuracy                           0.76      2980\n",
      "   macro avg       0.54      0.54      0.54      2980\n",
      "weighted avg       0.77      0.76      0.76      2980\n",
      "\n",
      "----------------------------------------\n",
      "CPU times: user 955 ms, sys: 185 ms, total: 1.14 s\n",
      "Wall time: 1.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bsmote = BorderlineSMOTE()\n",
    "test_oversample(X_train, y_train, X_test, y_test, bsmote, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57b1246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7684563758389261\n",
      "f1: 0.2159090909090909\n",
      "roc_auc_score: 0.534302694593503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      2544\n",
      "           1       0.21      0.22      0.22       436\n",
      "\n",
      "    accuracy                           0.77      2980\n",
      "   macro avg       0.54      0.54      0.54      2980\n",
      "weighted avg       0.77      0.77      0.77      2980\n",
      "\n",
      "----------------------------------------\n",
      "CPU times: user 38.7 s, sys: 340 ms, total: 39.1 s\n",
      "Wall time: 39.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svmsmote = SVMSMOTE()\n",
    "test_oversample(X_train, y_train, X_test, y_test, svmsmote, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01d0cf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7560402684563758\n",
      "f1: 0.22411953041622198\n",
      "roc_auc_score: 0.5305837444463678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      2544\n",
      "           1       0.21      0.24      0.22       436\n",
      "\n",
      "    accuracy                           0.76      2980\n",
      "   macro avg       0.54      0.54      0.54      2980\n",
      "weighted avg       0.77      0.76      0.76      2980\n",
      "\n",
      "----------------------------------------\n",
      "CPU times: user 1.05 s, sys: 99.8 ms, total: 1.14 s\n",
      "Wall time: 1.15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "adasyn = ADASYN()\n",
    "test_oversample(X_train, y_train, X_test, y_test, adasyn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b05db853",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_2 = tfidf.fit_transform(bow_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1af7c110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8530201342281879\n",
      "f1: 0.009049773755656108\n",
      "roc_auc_score: 0.7008025719808436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      2544\n",
      "           1       0.33      0.00      0.01       436\n",
      "\n",
      "    accuracy                           0.85      2980\n",
      "   macro avg       0.59      0.50      0.46      2980\n",
      "weighted avg       0.78      0.85      0.79      2980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f847f500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2540,    4],\n",
       "       [ 434,    2]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1957520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5023538531691747\n",
      "0.012798252598638246\n",
      "0.6752184222526221\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(model, tfidf_matrix_2, d2['label'], cv=5,scoring=('balanced_accuracy', 'f1', 'roc_auc'))\n",
    "print(scores['test_balanced_accuracy'].mean())\n",
    "print(scores['test_f1'].mean())\n",
    "print(scores['test_roc_auc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff903887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6959731543624161\n",
      "f1: 0.304147465437788\n",
      "roc_auc_score: 0.6535606355663262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.74      0.81      2544\n",
      "           1       0.23      0.45      0.30       436\n",
      "\n",
      "    accuracy                           0.70      2980\n",
      "   macro avg       0.56      0.60      0.55      2980\n",
      "weighted avg       0.79      0.70      0.73      2980\n",
      "\n",
      "----------------------------------------\n",
      "Accuracy: 0.7077181208053691\n",
      "f1: 0.3168627450980392\n",
      "roc_auc_score: 0.6528033220818186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81      2544\n",
      "           1       0.24      0.46      0.32       436\n",
      "\n",
      "    accuracy                           0.71      2980\n",
      "   macro avg       0.57      0.61      0.57      2980\n",
      "weighted avg       0.80      0.71      0.74      2980\n",
      "\n",
      "----------------------------------------\n",
      "Accuracy: 0.8402684563758389\n",
      "f1: 0.14388489208633093\n",
      "roc_auc_score: 0.6844968914084588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      2544\n",
      "           1       0.33      0.09      0.14       436\n",
      "\n",
      "    accuracy                           0.84      2980\n",
      "   macro avg       0.60      0.53      0.53      2980\n",
      "weighted avg       0.78      0.84      0.80      2980\n",
      "\n",
      "----------------------------------------\n",
      "Accuracy: 0.7006711409395974\n",
      "f1: 0.31278890600924497\n",
      "roc_auc_score: 0.6565195675379378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.74      0.81      2544\n",
      "           1       0.24      0.47      0.31       436\n",
      "\n",
      "    accuracy                           0.70      2980\n",
      "   macro avg       0.56      0.60      0.56      2980\n",
      "weighted avg       0.79      0.70      0.74      2980\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_oversample(X_train, y_train, X_test, y_test, smote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, bsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, svmsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, adasyn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2592309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14900, 2203)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vt_2 = vt1.fit_transform(tfidf_matrix_2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vt_2, d2['label'], test_size=0.2, random_state=42)\n",
    "tfidf_vt_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "591a6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7328859060402685\n",
      "f1: 0.2968197879858657\n",
      "roc_auc_score: 0.647198300732791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.84      2544\n",
      "           1       0.24      0.39      0.30       436\n",
      "\n",
      "    accuracy                           0.73      2980\n",
      "   macro avg       0.56      0.59      0.57      2980\n",
      "weighted avg       0.79      0.73      0.76      2980\n",
      "\n",
      "----------------------------------------\n",
      "Accuracy: 0.7369127516778523\n",
      "f1: 0.2974910394265233\n",
      "roc_auc_score: 0.653084609947493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84      2544\n",
      "           1       0.24      0.38      0.30       436\n",
      "\n",
      "    accuracy                           0.74      2980\n",
      "   macro avg       0.56      0.59      0.57      2980\n",
      "weighted avg       0.79      0.74      0.76      2980\n",
      "\n",
      "----------------------------------------\n",
      "Accuracy: 0.771476510067114\n",
      "f1: 0.26218851570964247\n",
      "roc_auc_score: 0.6527438188794645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      2544\n",
      "           1       0.25      0.28      0.26       436\n",
      "\n",
      "    accuracy                           0.77      2980\n",
      "   macro avg       0.56      0.57      0.56      2980\n",
      "weighted avg       0.78      0.77      0.78      2980\n",
      "\n",
      "----------------------------------------\n",
      "Accuracy: 0.7375838926174496\n",
      "f1: 0.30053667262969586\n",
      "roc_auc_score: 0.649399919219895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84      2544\n",
      "           1       0.25      0.39      0.30       436\n",
      "\n",
      "    accuracy                           0.74      2980\n",
      "   macro avg       0.56      0.59      0.57      2980\n",
      "weighted avg       0.79      0.74      0.76      2980\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_oversample(X_train, y_train, X_test, y_test, smote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, bsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, svmsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, adasyn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c0668b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(X.ravel()).apply(lambda x: \" \".join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c4c6fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_val(d1x, d1y, d2x, d2y, k):\n",
    "    output = []\n",
    "    \n",
    "    samples_per_class_in_test = 250  # Adjust as needed\n",
    "\n",
    "    # Initialize empty lists to store the train and test indices\n",
    "    train_indices1 = [[] for i in range(k)]\n",
    "    train_indices2 = [[] for i in range(k)]\n",
    "    test_indices1 = [[] for i in range(k)]\n",
    "    test_indices2 = [[] for i in range(k)]\n",
    "    \n",
    "    for class_label in [0,1]:\n",
    "        # Get the indices of samples belonging to the current class\n",
    "        class_indices1 = np.where(d1y == class_label)[0]\n",
    "        class_indices2 = np.where(d2y == class_label)[0]\n",
    "\n",
    "        # Randomly select samples_per_class_in_test samples from this class\n",
    "        selected_indices1 = np.random.choice(class_indices1, samples_per_class_in_test*k, replace=False)\n",
    "        selected_indices2 = np.random.choice(class_indices2, samples_per_class_in_test*k, replace=False)\n",
    "        \n",
    "        selected_indices1 = selected_indices1.reshape(k, samples_per_class_in_test)\n",
    "        selected_indices2 = selected_indices2.reshape(k, samples_per_class_in_test)\n",
    "        \n",
    "        for i in range(k):\n",
    "            test_indices1[i].extend(selected_indices1[i])\n",
    "            test_indices2[i].extend(selected_indices2[i])\n",
    "            \n",
    "\n",
    "            # Add the remaining samples to the train set indices\n",
    "            remaining_indices = np.setdiff1d(class_indices1, selected_indices1[i])\n",
    "            train_indices1[i].extend(remaining_indices)\n",
    "            \n",
    "            remaining_indices = np.setdiff1d(class_indices2, selected_indices2[i])\n",
    "            train_indices2[i].extend(remaining_indices)\n",
    "    print(len(train_indices1[0]))\n",
    "    print(len(train_indices2[0]))\n",
    "    print(type(d1x[train_indices1[0]]))\n",
    "    print(type(d1y[train_indices1[0]]))\n",
    "    for i in range(k):\n",
    "        # Split the data into train and test sets using the selected indices\n",
    "        output.append([d1x[train_indices1[i]].append(d2x[train_indices2[i]], ignore_index = True), d1x[test_indices1[i]].append(d2x[test_indices2[i]], ignore_index= True), d1y[train_indices1[i]].append(d2y[train_indices2[i]], ignore_index= True), d1y[test_indices1[i]].append(d2y[test_indices2[i]], ignore_index= True)])\n",
    "    \n",
    "    return output, len(train_indices1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14a5619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000\n",
      "14400\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "data, d1_len = get_k_val(d1['text'], d1['label'], d2['text'], d2['label'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32e792d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8c8c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_data = get_k_val(bow_matrix_1.toarray, d1['label'], bow_matrix_2, d2['label'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3059278",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "add23b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33400,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6fffdfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14400,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[19000:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "694f1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = X_train[19000:]\n",
    "y_train_2 = y_train[19000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb5ee679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c89cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33400,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc585234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e50f55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11400"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "708c80df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e366085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix_train = vectorizer.fit_transform(X_train.apply(lambda x: \" \".join(map(str, x))))\n",
    "bow_matrix_test = vectorizer.transform(X_test.apply(lambda x: \" \".join(map(str, x))))\n",
    "\n",
    "tfidf_matrix_train = tfidf.fit_transform(bow_matrix_train)\n",
    "tfidf_matrix_test = tfidf.transform(bow_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "129ea133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33400, 5000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "faca5f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19000, 5000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix_train[:19000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e8f211d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19000,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:19000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "307d591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix_train_2 = bow_matrix_train[19000:]\n",
    "y_train_2 = y_train[19000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ee19f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix_train_2_o, y_train_2_o = oversample.fit_resample(bow_matrix_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e60ce5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix_train_2_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a80ffdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_2_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c65ff0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "X_train_o = sp.sparse.vstack((bow_matrix_train[:19000], bow_matrix_train_2_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5a9dcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97a2f65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab276b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_o = np.append(y_train[:19000], y_train_2_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d67272b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a1021fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_o, y_train_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5cbe364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.724\n",
      "f1: 0.7006507592190889\n",
      "roc_auc_score: 0.776104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74       500\n",
      "           1       0.77      0.65      0.70       500\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.73      0.72      0.72      1000\n",
      "weighted avg       0.73      0.72      0.72      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set.\n",
    "y_pred = model.predict(bow_matrix_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(bow_matrix_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f37ce73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33400, 5000)\n",
      "(1000, 5000)\n",
      "(33400, 5000)\n",
      "(1000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(bow_matrix_train.shape)\n",
    "print(bow_matrix_test.shape)\n",
    "print(tfidf_matrix_train.shape)\n",
    "print(tfidf_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "825c3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, y_train, X_test, y_test, model, d1_len, oversample = None):\n",
    "    if oversample is not None:\n",
    "        \n",
    "        X_train_2_o, y_train_2_o = oversample.fit_resample(X_train[d1_len:], y_train[d1_len:])\n",
    "        X_train_o = sp.sparse.vstack((X_train[:d1_len], X_train_2_o))\n",
    "        y_train_o = np.append(y_train[:d1_len], y_train_2_o)\n",
    "        \n",
    "        model.fit(X_train_o, y_train_o)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set.\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "    print(f\"roc_auc_score: {roc_auc}\")\n",
    "    print(report)\n",
    "    print(\"-\"*50)\n",
    "    # Predict on the test set.\n",
    "    print('Domain1')\n",
    "    y_pred = model.predict(X_test[:500])\n",
    "    # Evaluate the model.\n",
    "    accuracy = accuracy_score(y_test[:500], y_pred)\n",
    "    f1 = f1_score(y_test[:500], y_pred)\n",
    "    roc_auc = roc_auc_score(y_test[:500], model.predict_proba(X_test[:500])[:, 1])\n",
    "    report = classification_report(y_test[:500], y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "    print(f\"roc_auc_score: {roc_auc}\")\n",
    "    print(report)\n",
    "    # Predict on the test set.\n",
    "    print(\"-\"*50)\n",
    "    print(\"Domain 2\")\n",
    "    \n",
    "    y_pred = model.predict(X_test[500:])\n",
    "    # Evaluate the model.\n",
    "    accuracy = accuracy_score(y_test[500:], y_pred)\n",
    "    f1 = f1_score(y_test[500:], y_pred)\n",
    "    roc_auc = roc_auc_score(y_test[500:], model.predict_proba(X_test[500:])[:, 1])\n",
    "    report = classification_report(y_test[500:], y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "    print(f\"roc_auc_score: {roc_auc}\")\n",
    "    print(report)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6b524e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.725\n",
      "f1: 0.6913580246913581\n",
      "roc_auc_score: 0.788752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75       500\n",
      "           1       0.79      0.62      0.69       500\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.74      0.72      0.72      1000\n",
      "weighted avg       0.74      0.72      0.72      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.908\n",
      "f1: 0.9122137404580152\n",
      "roc_auc_score: 0.963936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       250\n",
      "           1       0.87      0.96      0.91       250\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.542\n",
      "f1: 0.3760217983651226\n",
      "roc_auc_score: 0.550976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.81      0.64       250\n",
      "           1       0.59      0.28      0.38       250\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.56      0.54      0.51       500\n",
      "weighted avg       0.56      0.54      0.51       500\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model, d1_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "264340f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.718\n",
      "f1: 0.6948051948051948\n",
      "roc_auc_score: 0.776756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.74       500\n",
      "           1       0.76      0.64      0.69       500\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.72      0.72      0.72      1000\n",
      "weighted avg       0.72      0.72      0.72      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.892\n",
      "f1: 0.8941176470588236\n",
      "roc_auc_score: 0.94744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       250\n",
      "           1       0.88      0.91      0.89       250\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.89      0.89      0.89       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.544\n",
      "f1: 0.4492753623188406\n",
      "roc_auc_score: 0.546416\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61       250\n",
      "           1       0.57      0.37      0.45       250\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.55      0.54      0.53       500\n",
      "weighted avg       0.55      0.54      0.53       500\n",
      "\n",
      "--------------------------------------------------\n",
      "smote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.727\n",
      "f1: 0.6792009400705054\n",
      "roc_auc_score: 0.7762319999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76       500\n",
      "           1       0.82      0.58      0.68       500\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.75      0.73      0.72      1000\n",
      "weighted avg       0.75      0.73      0.72      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.908\n",
      "f1: 0.9094488188976377\n",
      "roc_auc_score: 0.96032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       250\n",
      "           1       0.90      0.92      0.91       250\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.546\n",
      "f1: 0.3381924198250729\n",
      "roc_auc_score: 0.534464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.86      0.65       250\n",
      "           1       0.62      0.23      0.34       250\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.58      0.55      0.50       500\n",
      "weighted avg       0.58      0.55      0.50       500\n",
      "\n",
      "--------------------------------------------------\n",
      "bsmote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.715\n",
      "f1: 0.6666666666666666\n",
      "roc_auc_score: 0.7742439999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.86      0.75       500\n",
      "           1       0.80      0.57      0.67       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.73      0.71      0.71      1000\n",
      "weighted avg       0.73      0.71      0.71      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.908\n",
      "f1: 0.9098039215686274\n",
      "roc_auc_score: 0.9586560000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       250\n",
      "           1       0.89      0.93      0.91       250\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.522\n",
      "f1: 0.3072463768115942\n",
      "roc_auc_score: 0.5324000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.83      0.64       250\n",
      "           1       0.56      0.21      0.31       250\n",
      "\n",
      "    accuracy                           0.52       500\n",
      "   macro avg       0.54      0.52      0.47       500\n",
      "weighted avg       0.54      0.52      0.47       500\n",
      "\n",
      "--------------------------------------------------\n",
      "svmsmote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721\n",
      "f1: 0.6729191090269637\n",
      "roc_auc_score: 0.783704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76       500\n",
      "           1       0.81      0.57      0.67       500\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.74      0.72      0.71      1000\n",
      "weighted avg       0.74      0.72      0.71      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.906\n",
      "f1: 0.9076620825147348\n",
      "roc_auc_score: 0.9615680000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       250\n",
      "           1       0.89      0.92      0.91       250\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.536\n",
      "f1: 0.3255813953488372\n",
      "roc_auc_score: 0.547088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.85      0.65       250\n",
      "           1       0.60      0.22      0.33       250\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.56      0.54      0.49       500\n",
      "weighted avg       0.56      0.54      0.49       500\n",
      "\n",
      "--------------------------------------------------\n",
      "adasyn\n",
      "Accuracy: 0.72\n",
      "f1: 0.6713615023474178\n",
      "roc_auc_score: 0.775856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76       500\n",
      "           1       0.81      0.57      0.67       500\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.74      0.72      0.71      1000\n",
      "weighted avg       0.74      0.72      0.71      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.904\n",
      "f1: 0.9047619047619047\n",
      "roc_auc_score: 0.9593280000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90       250\n",
      "           1       0.90      0.91      0.90       250\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.536\n",
      "f1: 0.33333333333333337\n",
      "roc_auc_score: 0.5323359999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.84      0.64       250\n",
      "           1       0.59      0.23      0.33       250\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.56      0.54      0.49       500\n",
      "weighted avg       0.56      0.54      0.49       500\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "print(\"oversample\")\n",
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model, d1_len, oversample)\n",
    "print(\"smote\")\n",
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model, d1_len, smote)\n",
    "print(\"bsmote\")\n",
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model, d1_len, bsmote)\n",
    "print(\"svmsmote\")\n",
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model, d1_len, svmsmote)\n",
    "print(\"adasyn\")\n",
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model, d1_len, adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3c58df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.734\n",
      "f1: 0.6921296296296295\n",
      "roc_auc_score: 0.823184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.87      0.77       500\n",
      "           1       0.82      0.60      0.69       500\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.75      0.73      0.73      1000\n",
      "weighted avg       0.75      0.73      0.73      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.922\n",
      "f1: 0.9265536723163841\n",
      "roc_auc_score: 0.967344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       250\n",
      "           1       0.88      0.98      0.93       250\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.93      0.92      0.92       500\n",
      "weighted avg       0.93      0.92      0.92       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.546\n",
      "f1: 0.3183183183183183\n",
      "roc_auc_score: 0.591632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.88      0.66       250\n",
      "           1       0.64      0.21      0.32       250\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.58      0.55      0.49       500\n",
      "weighted avg       0.58      0.55      0.49       500\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model, d1_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4afe1ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.734\n",
      "f1: 0.7217573221757322\n",
      "roc_auc_score: 0.832884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75       500\n",
      "           1       0.76      0.69      0.72       500\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.74      0.73      0.73      1000\n",
      "weighted avg       0.74      0.73      0.73      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.896\n",
      "f1: 0.896414342629482\n",
      "roc_auc_score: 0.95656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90       250\n",
      "           1       0.89      0.90      0.90       250\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.572\n",
      "f1: 0.5286343612334801\n",
      "roc_auc_score: 0.618528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.66      0.61       250\n",
      "           1       0.59      0.48      0.53       250\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.57      0.57      0.57       500\n",
      "\n",
      "--------------------------------------------------\n",
      "smote\n",
      "Accuracy: 0.735\n",
      "f1: 0.7177848775292865\n",
      "roc_auc_score: 0.833816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75       500\n",
      "           1       0.77      0.67      0.72       500\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.74      0.74      0.73      1000\n",
      "weighted avg       0.74      0.73      0.73      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.894\n",
      "f1: 0.8950495049504951\n",
      "roc_auc_score: 0.9597600000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       250\n",
      "           1       0.89      0.90      0.90       250\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.89      0.89      0.89       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.576\n",
      "f1: 0.5115207373271888\n",
      "roc_auc_score: 0.6102239999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.71      0.63       250\n",
      "           1       0.60      0.44      0.51       250\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.58      0.58      0.57       500\n",
      "weighted avg       0.58      0.58      0.57       500\n",
      "\n",
      "--------------------------------------------------\n",
      "bsmote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744\n",
      "f1: 0.7276595744680852\n",
      "roc_auc_score: 0.835852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76       500\n",
      "           1       0.78      0.68      0.73       500\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.75      0.74      0.74      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.908\n",
      "f1: 0.9094488188976377\n",
      "roc_auc_score: 0.9601919999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       250\n",
      "           1       0.90      0.92      0.91       250\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.58\n",
      "f1: 0.513888888888889\n",
      "roc_auc_score: 0.612208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.72      0.63       250\n",
      "           1       0.61      0.44      0.51       250\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.59      0.58      0.57       500\n",
      "weighted avg       0.59      0.58      0.57       500\n",
      "\n",
      "--------------------------------------------------\n",
      "svmsmote\n",
      "Accuracy: 0.74\n",
      "f1: 0.7117516629711752\n",
      "roc_auc_score: 0.833844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76       500\n",
      "           1       0.80      0.64      0.71       500\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.75      0.74      0.74      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.902\n",
      "f1: 0.9033530571992111\n",
      "roc_auc_score: 0.960032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       250\n",
      "           1       0.89      0.92      0.90       250\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.578\n",
      "f1: 0.46582278481012657\n",
      "roc_auc_score: 0.6047199999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.79      0.65       250\n",
      "           1       0.63      0.37      0.47       250\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.59      0.58      0.56       500\n",
      "weighted avg       0.59      0.58      0.56       500\n",
      "\n",
      "--------------------------------------------------\n",
      "adasyn\n",
      "Accuracy: 0.743\n",
      "f1: 0.7274655355249205\n",
      "roc_auc_score: 0.8337720000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76       500\n",
      "           1       0.77      0.69      0.73       500\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.75      0.74      0.74      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.9\n",
      "f1: 0.9011857707509882\n",
      "roc_auc_score: 0.9585440000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       250\n",
      "           1       0.89      0.91      0.90       250\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.586\n",
      "f1: 0.5263157894736843\n",
      "roc_auc_score: 0.610352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.71      0.63       250\n",
      "           1       0.61      0.46      0.53       250\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.59      0.59      0.58       500\n",
      "weighted avg       0.59      0.59      0.58       500\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainstyle/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "print(\"oversample\")\n",
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model, d1_len, oversample)\n",
    "print(\"smote\")\n",
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model, d1_len, smote)\n",
    "print(\"bsmote\")\n",
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model, d1_len, bsmote)\n",
    "print(\"svmsmote\")\n",
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model, d1_len, svmsmote)\n",
    "print(\"adasyn\")\n",
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model, d1_len, adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d60d4c",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(d2['label']), y=d2['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622dc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oversample(X_train, y_train, X_test, y_test, smote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, bsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, svmsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, adasyn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vt_2, d2['label'], test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oversample(X_train, y_train, X_test, y_test, smote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, bsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, svmsmote, model)\n",
    "test_oversample(X_train, y_train, X_test, y_test, adasyn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bow_matrix_train, y_train, bow_matrix_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c136d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "param={'solver': ['lbfgs', 'sag','newton-cholesky'],\n",
    "       'c':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
    "best_score = 0\n",
    "best_solver = ''\n",
    "best_c = 0\n",
    "for s in param['solver']:\n",
    "    for c in param['c']:\n",
    "        lr = LogisticRegression(C = c, solver = s, class_weight='balanced')\n",
    "        lr.fit(bow_matrix_train, y_train)\n",
    "        result = lr.score(bow_matrix_test, y_test)\n",
    "        print(\"Solver: \"+str(s)+\", c: \"+str(c)+\", accurancy: \" + str(result))\n",
    "        if result > best_score:\n",
    "            best_score = result\n",
    "            best_solver = s\n",
    "            best_c = c\n",
    "lr =LogisticRegression(C = best_c, solver = best_solver, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: solver = ' +str(best_solver)+ \", c: \"+str(best_c)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "param={'solver': ['lbfgs', 'sag','newton-cholesky'],\n",
    "       'c':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
    "best_score = 0\n",
    "best_solver = ''\n",
    "best_c = 0\n",
    "for s in param['solver']:\n",
    "    for c in param['c']:\n",
    "        lr = LogisticRegression(C = c, solver = s, class_weight='balanced')\n",
    "        lr.fit(tfidf_matrix_train, y_train)\n",
    "        result = lr.score(tfidf_matrix_test, y_test)\n",
    "        print(\"Solver: \"+str(s)+\", c: \"+str(c)+\", accurancy: \" + str(result))\n",
    "        if result > best_score:\n",
    "            best_score = result\n",
    "            best_solver = s\n",
    "            best_c = c\n",
    "lr =LogisticRegression(C = best_c, solver = best_solver, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: solver = ' +str(best_solver)+ \", c: \"+str(best_c)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1669f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e44973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train_o, y_train_o = oversample.fit_resample(bow_matrix_train, y_train)\n",
    "    \n",
    "param={'solver': ['lbfgs', 'sag','newton-cholesky'],\n",
    "       'c':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
    "best_score = 0\n",
    "best_solver = ''\n",
    "best_c = 0\n",
    "for s in param['solver']:\n",
    "    for c in param['c']:\n",
    "        lr = LogisticRegression(C = c, solver = s, class_weight='balanced')\n",
    "        lr.fit(X_train_o, y_train_o)\n",
    "        result = lr.score(bow_matrix_test, y_test)\n",
    "        print(\"Solver: \"+str(s)+\", c: \"+str(c)+\", accurancy: \" + str(result))\n",
    "        if result > best_score:\n",
    "            best_score = result\n",
    "            best_solver = s\n",
    "            best_c = c\n",
    "lr =LogisticRegression(C = best_c, solver = best_solver, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: solver = ' +str(best_solver)+ \", c: \"+str(c)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train_o, y_train_o = oversample.fit_resample(tfidf_matrix_train, y_train)\n",
    "    \n",
    "param={'solver': ['lbfgs', 'sag','newton-cholesky'],\n",
    "       'c':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
    "best_score = 0\n",
    "best_solver = ''\n",
    "best_c = 0\n",
    "for s in param['solver']:\n",
    "    for c in param['c']:\n",
    "        lr = LogisticRegression(C = c, solver = s, class_weight='balanced')\n",
    "        lr.fit(X_train_o, y_train_o)\n",
    "        result = lr.score(tfidf_matrix_test, y_test)\n",
    "        print(\"Solver: \"+str(s)+\", c: \"+str(c)+\", accurancy: \" + str(result))\n",
    "        if result > best_score:\n",
    "            best_score = result\n",
    "            best_solver = s\n",
    "            best_c = c\n",
    "lr =LogisticRegression(C = best_c, solver = best_solver, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: solver = ' +str(best_solver)+ \", c: \"+str(c)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ddd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "alpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "best_score = 0\n",
    "best_alpha = 0\n",
    "for a in alpha:\n",
    "    \n",
    "    rr = RidgeClassifier(alpha = a, class_weight='balanced')\n",
    "    rr.fit(bow_matrix_train, y_train)\n",
    "    result = rr.score(bow_matrix_test, y_test)\n",
    "    print(\"alpha: \"+str(a)+\", accurancy: \" + str(result))\n",
    "    if result > best_score:\n",
    "        best_score = result\n",
    "        best_alpha = a\n",
    "lr =RidgeClassifier(alpha = best_alpha, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: alpha = ' +str(best_alpha)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68551eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "alpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "best_score = 0\n",
    "best_alpha = 0\n",
    "for a in alpha:\n",
    "    \n",
    "    rr = RidgeClassifier(alpha = a, class_weight='balanced')\n",
    "    rr.fit(tfidf_matrix_train, y_train)\n",
    "    result = rr.score(tfidf_matrix_test, y_test)\n",
    "    print(\"alpha: \"+str(a)+\", accurancy: \" + str(result))\n",
    "    if result > best_score:\n",
    "        best_score = result\n",
    "        best_alpha = a\n",
    "lr =RidgeClassifier(alpha = best_alpha, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: alpha = ' +str(best_alpha)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train_o, y_train_o = oversample.fit_resample(bow_matrix_train, y_train)\n",
    "    \n",
    "alpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "best_score = 0\n",
    "best_alpha = 0\n",
    "for a in alpha:\n",
    "    \n",
    "    rr = RidgeClassifier(alpha = a, class_weight='balanced')\n",
    "    rr.fit(X_train_o, y_train_o)\n",
    "    result = rr.score(bow_matrix_test, y_test)\n",
    "    print(\"alpha: \"+str(a)+\", accurancy: \" + str(result))\n",
    "    if result > best_score:\n",
    "        best_score = result\n",
    "        best_alpha = a\n",
    "lr =RidgeClassifier(alpha = best_alpha, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: alpha = ' +str(best_alpha)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train_o, y_train_o = oversample.fit_resample(tfidf_matrix_train, y_train)\n",
    "    \n",
    "alpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "best_score = 0\n",
    "best_alpha = 0\n",
    "for a in alpha:\n",
    "    \n",
    "    rr = RidgeClassifier(alpha = a, class_weight='balanced')\n",
    "    rr.fit(X_train_o, y_train_o)\n",
    "    result = rr.score(tfidf_matrix_test, y_test)\n",
    "    print(\"alpha: \"+str(a)+\", accurancy: \" + str(result))\n",
    "    if result > best_score:\n",
    "        best_score = result\n",
    "        best_alpha = a\n",
    "lr =RidgeClassifier(alpha = best_alpha, class_weight='balanced')\n",
    "lr.fit(bow_matrix_train, y_train)\n",
    "print('best param: alpha = ' +str(best_alpha)+\" with accuracy:\" + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b489a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57de2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 5, 8)\n",
    "gamma_range = np.logspace(-6, 1, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_matrix_1, d1['label'], test_size=0.2, random_state=42)\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "#roc_auc = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "#print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80776c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "#roc_auc = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "#print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "#roc_auc = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "#print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6356eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(bow_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "svm = SVC()\n",
    "svm.fit(X_train_o, y_train_o)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "#roc_auc = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "#print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(bow_matrix_2, d2['label'], test_size=0.2, random_state=42)\n",
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(X_train_o, y_train_o)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "#roc_auc = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")\n",
    "#print(f\"roc_auc_score: {roc_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c56bc502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.719\n",
      "f1: 0.6364812419146184\n",
      "roc_auc_score: 0.8153880000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.95      0.77       500\n",
      "           1       0.90      0.49      0.64       500\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.78      0.72      0.70      1000\n",
      "weighted avg       0.78      0.72      0.70      1000\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain1\n",
      "Accuracy: 0.934\n",
      "f1: 0.9349112426035503\n",
      "roc_auc_score: 0.978736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93       250\n",
      "           1       0.92      0.95      0.93       250\n",
      "\n",
      "    accuracy                           0.93       500\n",
      "   macro avg       0.93      0.93      0.93       500\n",
      "weighted avg       0.93      0.93      0.93       500\n",
      "\n",
      "--------------------------------------------------\n",
      "Domain 2\n",
      "Accuracy: 0.504\n",
      "f1: 0.06766917293233081\n",
      "roc_auc_score: 0.579504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66       250\n",
      "           1       0.56      0.04      0.07       250\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.53      0.50      0.36       500\n",
      "weighted avg       0.53      0.50      0.36       500\n",
      "\n",
      "--------------------------------------------------\n",
      "CPU times: user 54min 6s, sys: 40.1 ms, total: 54min 6s\n",
      "Wall time: 49min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(tfidf_matrix_train, y_train, tfidf_matrix_test, y_test, SVC(class_weight='balanced', probability = True), d1_len, svmsmote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
